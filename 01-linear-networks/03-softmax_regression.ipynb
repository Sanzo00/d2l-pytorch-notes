{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2593711e",
   "metadata": {},
   "source": [
    "## 分类问题\n",
    "\n",
    "为了估计每个类别的条件概率，需要一个有多个输出的模型，每个类别对应一个输出。\n",
    "\n",
    "对应于线性回归，分类问题需要和输出一样多的仿射函数(affine function)，假设有4个特征，3个可能的输出类别，那么将需要12个标量表示权重，3个标量表示偏置。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n",
    "o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n",
    "o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![](./img/softmaxreg.png)\n",
    "\n",
    "向量的表示形式为$\\mathbf{o} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4c4cb",
   "metadata": {},
   "source": [
    "## softmax\n",
    "\n",
    "$\\mathrm{softmax}$是将预测的概率进行归一化，然后取最大的概率对应的类别作为预测的结果。\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\text{其中}\\quad  \\hat{y}_j = \\frac{exp(o_j)}{\\sum_k exp(o_k)}$$\n",
    "\n",
    "\n",
    "$$\\operatorname*{argmax}_j \\hat{y}_j = \\operatorname*{argmax}_j o_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc1a29",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "$\\mathrm{softmax}$得到向量$\\mathbf{\\hat{y}}$，可以将其视为给定$\\mathbf{x}$的每个类别的估计条件概率。\n",
    "\n",
    "$$P(\\mathbf{Y}\\mid \\mathbf{X})= \\prod_{i=1}^n P(\\mathbf{y^{(i)} \\mid \\mathbf{x}^{(i)}}).$$\n",
    "\n",
    "根据最大似然估计，最大化$P(\\mathbf{Y} \\mid \\mathbf{X})$相当于最小化负对数似然：\n",
    "\n",
    "$$\n",
    "-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n",
    "$$\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aba442",
   "metadata": {},
   "source": [
    "## softmax及其导数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n",
    "&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\\n",
    "&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "为了更好地理解发生了什么，考虑相对于任何未归一化的预测 $o_j$ 的导数。我们得到：\n",
    "\n",
    "$$\n",
    "\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n",
    "$$\n",
    "\n",
    "误差函数的导数刚好对应$\\mathrm{softmax}$函数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
